\chapter{Discussion}
In this chapter, a discussion is held about the graph representation as well as the three studied cases. The chapter is concluded with a brief discussion about future work.

\section{Gh0st RAT Controllers}
% In discussion - what similarity measure was the most appropriate to use in our case?
Based on the results, the simple local indices perform very well on both of the two data sets. The F$_1$ score seem to increase with the number of reference nodes which seems reasonable since the the change of a high recall is correlated with having reference nodes well distributed in the network. The change of getting a good distribution gets higher the more reference nodes. 

It may be surprising that the classifier based on local similarity generates such good results. However, it has to do with the graph itself, and how it is modelled. One reason for the good results is the the density of the graph. The local similarity measures are based solely on the nearest neighbors implying that all of the RAT controllers must share one common neighbor with one of the known RAT controller from the start in order to be found. If one (unclassified) RAT controller is positioned in the periphery of the network, far away from the reference nodes, that will never be properly classified. Hence, using local similarity measures for classification is highly dependent of the density of the graph and/or a good distribution of the reference nodes in the graph. 

The high density of the graph can also explain the fact why the Local Path index and the Katz index does not perform as well as the local index. Including more neighbors to account for the similarity makes the method sensitive to finding the best threshold. If the threshold is badly chosen, it will either result in bad precision or bad recall resulting in a poor accuracy.

% Falsely annotated IP addresses
In the first dataset, three IP addresses annotated as \textit{Non-ghost}s were classified as \textit{Ghost} in the majority of the cases. There is reason to believe those IP addresses are falsely annotated and are in fact RAT controllers. [How can we prove it or make this claim stronger?]

To conclude, we find that the local indices perform very well in this particular case of classifying IP addresses based on a graph modelled from NetFlow data. They are robust in the sense that they are not sensitive to the choice of threshold, which can be set to zero and still output good performance. The five different local indices that were tried does not vary much in performance and thus any of them can be recommended to be used. This method could be used both for binary classification of unknown IP addresses where only a few are known to belong to a certain class. Furthermore, it can be used to identify falsely annotated IP addresses. 

\section{Malicious IP addresses}
All of the three SVM models perform far better classification than a random classifier. The model with 4 features performs better than the one using only 2 nodes, however, the best one is the combination, when all 6 features are used. 

The SVM classifier seems to have a bias towards the lower risk classes, for all of the three SVMs. This may very well have to do with the dataset being extremely skewed. We see that all of the three SVMs perform far less higher classifications than the random classifier and more lower classifications than the random classifier.

% A lot of noise in the dataset
One reason for the low result may very well have to do with the noise in the dataset. In this context, noise is referred to as IP addresses that by Recorded Future's rule based system would have been classified with a high risk class however, have an exception. One such example is Google's DNS server, which have been set to a risk class of 1 since it is not regarded to be a malicious source. 

It is hard to quantify the amount of noise present in the data, however it is only reasonable to believe it has an impact on the results. We can thus derive some of the poor classifications to noise. 

Although an accuracy of 75 \% can be argued to be rather good, it is not by far good enough serve as an independent classification system of Recorded Future and we can recognize the fact that we were not able to reconstruct Recorded Future's current classification system. However, with some modifications, where more features are included, we believe the performance can be increased. Even though it may be hard to reach an accuracy sufficiently high to accept it as a valid classification system at Recorded Future, this kind of methods could serve as an learning tool, leading to many important insights for Recorded Future. Not only could it tell them a lot about their classification system today, but also drive insights towards what kind of features or information that are the most important when classifying IP addresses.

% How can the classifier be improved?
It should be mentioned that the aim was to include two more features, namely the betweenness centrality and the closeness centrality. The calculations of the centrality measures were started in Cypher in Neo4j, however, the large amount of data resulted in a very long runtime, and thus the conclusion that the network was too big for these measures was drawn. The betweenness and closeness centrality might have contributed to better predictions but were excluded from our model.

\section{Prediction of Future Cyber Attacks}
\input{chapters/cyberattacks_discussion.tex}

\section{Future Work}
This report has only covered a small part of interesting methods based on graph theory that might be of interest to apply to the cyber threat intelligence domain. Thus, we now present some interesting ideas that was stumbled upon without having the resources to try them out. 

In the Gh0st RAT controller case, structural equivalence was used for classification of nodes. However, there is another similarity called regular equivalence that takes the position of the node into consideration. Thus, two nodes are regularly equivalent if they are equally related to equivalent others. Although structural equivalence measure might have been enough for the RAT controller case, there are many different areas where regular similarity would suit better. However, the disadvantage of regular equivalence it that it implies an iterative or recursive nature since the similarity between the neighborhoods of the nodes has to be known before the similarity of the nodes themselves can be computed \cite{leicht2006}. Thus, the regular equivalence involves a higher complexity than many of the structural equivalence measures, howver it could lead to new insights. 

In this study, we have selected only a few features for our classifier. We have also limited our reseach to only account for a very small amount of all the information available in Recorded Future's database. Thus, a natural extension of this research may be to include other and more features in the classification model. Such studies could lead to more insights about Recorded Future's current classification system.

The PLP was only evaluated on the data set of cyber attacks but might as well be applicable to other data set that can be naturally represented by a bipartite graph. One example is terror attacks and another is terror financing, but any other scenario might also be of interest.

Another interesting idea is to mine the graph for often occurring sub-structures. This might be done with a more exploratory interest in mind. One algorithm for such a purpose is the AProximate Graph Mining (APGM) developed by \citet{Jia2011}, that looks for often occurring structures but accounts for noise and perturbations in the original data. This can be of high interest for Recorded Future because of the uncertain nature of the data, covered in \secref{dataset}.
