\chapter{Discussion}
In this chapter, a discussion is held about the graph representation as well as the three studied cases. The chapter is concluded with suggestions of future cases and analyses that could be of interest to Recorded Future.

\section{Extracting Relations From a Database}
% Power of graph databases
We are dealing with connected data, where choosing a graph database has two main advantages according to \citet{robinson2013}: performance and flexibility. In a relational database, the join-intensive query performance deteriorates as the dataset gets larger, with a graph database the performance remains relatively constant. This is due to the fact that a query is only localized to a fraction of the graph, making the run time proportional to the size of the sub-graph related to the query, rather than the overall graph containing all available data.

Furthermore, there is the aspect of flexibility. Graphs are naturally additive \cite{robinson2013} such that new sub-graphs can be added containing more nodes and edges and new relations can be introduced as the understanding of the dataset grows. This have a positive implication for the analysts that does not have to model the domain in exhaustive detail from the start but can add information as time progress.

\section{Detection of Gh0st RAT Controllers}
% In discussion - what similarity measure was the most appropriate to use in our case?
Based on the results, the simple local indices perform very well on both of the two data sets. The F$_1$ score seems to increase with the number of reference nodes which is reasonable since the change of a high recall is correlated with having reference nodes well distributed in the network. The chance of getting a good distribution gets higher the more reference nodes we have. 

It may be surprising that the classifier based on local similarity generates such good results. However, it has to do with the graph itself, and how it is modelled. One reason for the good results is the density of the graph. The local similarity measures are based solely on the nearest neighbors implying that all of the RAT controllers must share one common neighbor with one of the known RAT controller from the start in order to be found. If one (unclassified) RAT controller is positioned in the periphery of the network, far away from the reference nodes, that will never be properly classified. Hence, using local similarity measures for classification is highly dependent of the density of the graph and/or a good distribution of the reference nodes in the graph. 

The high density of the graph can also explain the fact why the Local Path index and the Katz index do not perform as well as the local index. Including more neighbors to account for the similarity makes the method sensitive to finding the best threshold. If the threshold is badly chosen, it will either result in bad precision or bad recall resulting in a poor accuracy.

% Incorrectly annotated IP addresses
In the first dataset, three IP addresses annotated as \textit{Non-gh0st}s were classified as \textit{Gh0st} in the majority of the cases. There is reason to believe those IP addresses are incorrectly annotated and are in fact RAT controllers. This was confirmed by a security expert at Recorded Future.

If we handle the three incorrectly annotated IP addresses as RAT controllers, we find that we get an accuracy of 100\% when including 3 or more reference nodes trying to classify nodes using similarity. 

To conclude, we find that the local indices perform very well in this particular case of classifying IP addresses based on a graph modelled from NetFlow data. They are robust in the sense that they are not sensitive to the choice of threshold, which can be set to zero and still output good performance. The five different local indices that were tried do not vary much in performance and thus any of them can be recommended to be used. This method could be used both for binary classification of unknown IP addresses where only a few are known to belong to a certain class. Furthermore, it can be used to identify incorrectly annotated IP addresses. 

\section{Classification of Malicious IP Addresses}
All of the three SVM models perform far better classification than a random classifier. The model with 4 features performs better than the one using only 2 features, however, the best one (56\%) is the combination, when all 6 features are used. 

% A lot of noise in the dataset
One reason for the low result may very well have to do with the noise in the dataset. In this context, noise is referred to as IP addresses that by Recorded Future's rule based system would have been classified with a high risk class however, have an exception. One such example is Google's DNS server, which have been set to a risk class of 1 since it is not regarded to be a malicious source. 

It is hard to quantify the amount of noise present in the data, however it is only reasonable to believe it has an impact on the results. We can thus attribute some of the poor classifications to noise. However, as a future study, it would be interesting to compare any bad classifications with a list of IP address exceptions, such as the Google DNS server. 

Although an accuracy of 56\% can be argued to be rather good if one compares it to a random classifier, it is not by far good enough to serve as an independent classification system of Recorded Future and we can recognize the fact that we were not able to reconstruct Recorded Future's current classification system. However, with some modifications, where more features are included, we believe the performance can be increased. Even though it may be hard to reach an accuracy sufficiently high to accept it as a valid classification system at Recorded Future, this kind of methods could serve as a learning tool, leading to many important insights for Recorded Future. Not only could it tell a lot about the classification system today, but also drive insights towards what kind of features or information that are the most important when classifying IP addresses. In that way, new rules could be added of perhaps old ones made less significant. 

In figure \ref{accuracyPerClass}, it was found that class 2 got the worst accuracy in all of the three SVMs. That class 2 is consistently hard to classify could be an indication that class 2 is not well defined in the class space, at least given the features included in our models. It seems reasonable that class 2 is somewhat ambiguous since it represents the risk class \textit{Unusual}, which indicates a very small risk but not clearly a threat. Thus, all IP addresses that are not completely safe and not imply a threat are given risk class 2.

The unbalance in data did have a big effect on the results. If all class weights were set to 1, the classifier managed to perform with an accuracy of ~75\%, by classifying the majority of the datapoints as class 1 or 2. This lead us back to the discussion about high accuracy for unbalanced data, held in \secref{unbalanced}, and does not imply a good classifier. Instead, this shows the importance of handling the unbalanced data properly. Thus, as a future work, it would be of interest to apply other ways (such as oversampling methods or hybrid ones \citep{wang2014}) of handling unbalanced data, in hope of finding a better classifier. 

In this study, we have selected only a few (2-6) features for our classifiers. We have also limited our research to only account for a very small amount of all the information available in Recorded Future's database. Thus, a natural extension of this research may be to include other and more features in the classification model. 

It should be mentioned that the aim was to include two more features, namely the betweenness centrality and the closeness centrality. The calculations of the centrality measures were started in Cypher in Neo4j, however, the large amount of data resulted in a very long runtime (> two weeks using the Cypher-shell), and thus the conclusion that the network was too big for these measures was drawn. The betweenness and closeness centrality might have contributed to better predictions but were excluded from our model.


\section{Prediction of Future Cyber Attacks}
\input{chapters/cyberattacks_discussion.tex}

\section{Suggestion of Future Cases}
This report has only covered a small part of interesting methods based on graph theory that might be of interest to apply to the cyber threat intelligence domain. Thus, we now present some interesting ideas that we considered without having the resources to try them. 

In the Gh0st RAT controller case, structural equivalence was used for classification of nodes. However, there is another similarity called regular equivalence that takes the position of the node into consideration. Thus, two nodes are regularly equivalent if they are equally related to equivalent others. Although structural equivalence measure might have been enough for the RAT controller case, there are many different areas where regular similarity would work better. However, the disadvantage of regular equivalence is that it implies an iterative or recursive nature since the similarity between the neighborhoods of the nodes has to be known before the similarity of the nodes themselves can be computed \cite{leicht2006}. Thus, the regular equivalence involves a higher complexity than many of the structural equivalence measures, however it could lead to new insights. 

The PLP was only evaluated on the data set of cyber attacks but might as well be applicable to other data set that can be naturally represented by a bipartite graph. One example is terror attacks and another is terror financing, but many other scenarios might also be of interest.

Another interesting idea is to mine the graph for often occurring sub-structures. This might be done with a more exploratory interest in mind. One algorithm for such a purpose is the AProximate Graph Mining (APGM) developed by \citet{Jia2011}, that looks for often occurring structures but accounts for noise and perturbations in the original data. This can be of high interest for Recorded Future because of the uncertain nature of the data, covered in \secref{dataset}.
