\chapter{Theoretical Framework}
Mining data in a network is not a new subject. In fact, many different approaches have been suggested. It this section, we present some of them as the foundation for our project.

\section{Graphs Representation}
\input{chapters/graphs}

\section{Link Prediction in Bipartite Graphs}
\input{chapters/bipartite_link_prediction}

\section{Similarities Between Nodes \label{sim}}
Quantifying the similarities between vertexes in a network can be of great interest. In many situations it is useful to be able to determine what other nodes that are similar to a specific node. However, two nodes can be similar in many different ways. For instance, they may have the same amount of degrees, the same neighbour or both be a part of the same community. Thus, there are a numerous approaches to take when defining a similarity measures. Below we present a number of them. 

\subsection{Structural equivalence}
% How is structural equivalent defined?
Two vertexes are said to be structurally equivalent if they share many neighbors \cite{leicht2006}. Taken into a social network perspective, it seems reasonable to think that two persons have something in common of they have many common friends. 

Local similarity measures exploit the local structures of an undirected graph $G$ \cite{fouss2016algorithms}. Let $\Gamma_i$ be the neighborhood of node $i$ in a network. The common friends of node $i$ and $j$ is thus given by
\begin{equation}
\label{common}
\sigma_{common} = |\Gamma_i \cap \Gamma_j| = \sum_{k=1}^n a_{ik}a_{kj}
\end{equation}
where $|x|$ refers to the cardinality of the set $x$ such that $|\Gamma_x |$ gives the degree of node $x$. The expression above is the basics of the following similarity measures that will be presented. The expressions will be presented both from the notation of neighbours $\Gamma$ and also the adjacency matrix $\textbf{A}$.

\textbf{Cosine coefficient.} In an undirected graph, the cosine coefficient is simply the normalization of the score of same neighbors between node $i$ and $j$ \cite{fouss2016algorithms}:
\begin{equation}
    \label{cosine}
    \sigma_{cosine} = \frac{|\Gamma_i \cap \Gamma_j|}{\sqrt{|\Gamma_i||\Gamma_j|}} = \frac{\sum_{k=1}^n a_{ik}a_{kj}}{\sqrt{a_{i \circ }a_{\circ j}}}
\end{equation}
$a_{i \circ }$ denotes the summation of row $i$ in the adjacency matrix while $a_{\circ j}$ denoted the column $j$ of the adjacency matrix.

Generally speaking, the cosine coefficient between two node vectors $\textbf{v}_i$ and $\textbf{v}_j$ defined by $\textbf{v}_i^T\textbf{v}_i/(\|\textbf{v}_i\|\|\textbf{v}_j\|)$ is a measure of the linear relationship between the two node vectors. Thus, the the nodes are considered more similar the smaller angle between the node vectors in the node space.

\textbf{Jaccard index.} The Jaccard index is computed as follows:
\begin{equation}
    \label{jaccard}
    \sigma_{Jaccard} = \frac{|\Gamma_i \cap \Gamma_j|}{|\Gamma_i \cup \Gamma_j|} = \frac{\sum_{k=1}^n a_{ik}a_{kj}}{a_{i \circ }+a_{\circ j}-\sum_{k=1}^n a_{ik}a_{kj}}
\end{equation}
The denominator represents the number of neighbors belonging to at least one of the two nodes $i$ or $j$. Thus, the Jaccard index is the fraction of common neighbors in relation to the cardinality of the union of neighbors. 

\textbf{Dice coefficient.} The Dice coefficient is defined accordingly:
\begin{equation}
    \label{dice}
    \sigma_{Dice} = \frac{2 |\Gamma_i \cap \Gamma_j|}{|\Gamma_i|+|\Gamma_j|}= \frac{2\sum_{k=1}^n a_{ik}a_{kj}}{a_{i \circ }+a_{\circ j}}
\end{equation}
Defined in words, the Dice coefficient is calculated as twice the number of common neighbors divided by the sum of the cardinalities of the two neighborhoods. 

\textbf{Hub-promoted index.} The index promotes hubs in the network and is given by: 
\begin{equation}
    \label{prohub}
    \sigma_{prohub} = \frac{|\Gamma_i \cap \Gamma_j|}{\min(|\Gamma_i|,|\Gamma_j|)} = \frac{\sum_{k=1}^n a_{ik}a_{kj}}{\min(a_{i \circ },a_{\circ j})}
\end{equation}
Since the denominator is based on the lower degree only, the links adjacent to hubs are likely to be assigned high scores \citep{lu2011}. The measure is also called overlap similarity \citep{fouss2016algorithms} since a perfect score of 1 implies that the neighborhood of one of the nodes is a subset of the neighborhood of the other node.

\textbf{Hub-depressed index.} The hub-depressed index depresses hubs in the network \citep{fouss2016algorithms}:
\begin{equation}
    \label{dehub}
    \sigma_{dehub} = \frac{|\Gamma_i \cap \Gamma_j|}{\max(|\Gamma_i|,|\Gamma_j|)} = \frac{\sum_{k=1}^n a_{ik}a_{kj}}{\max(a_{i \circ },a_{\circ j})}
\end{equation}
In contrast to the denominator in the hub-promoted index, this denominator takes the higher degree into consideration. Thus, links adjacent to hubs are likely to be assigned low scores. 

\begin{comment}
\textbf{Adamic index.} 
\begin{equation}
    \label{adamic}
    \sigma_{Adamic} = \frac{|\Gamma_i \cap \Gamma_j|}{\log(|\Gamma_i|,|\Gamma_j|)} = \frac{\sum_{k=1}^n a_{ik}a_{kj}}{\max(a_{i \circ },a_{\circ j})}
\end{equation}
\end{comment}

A global similarity measure takes the topology of the whole graph into account. Comparing with the local indices, the global ones can give more accurate predictions \citep{lu2011}.

\textbf{Katz Index.} One global index is the Katz index, defined below \citep{fouss2016algorithms}.
\begin{equation}
    \textbf{K}_{Katz}=\sum_{t=1}^{\infty} \alpha^t \textbf{A}^t = (\textbf{I}-\alpha \textbf{A})^{-1}-\textbf{I}
\end{equation}
The parameter $\alpha \in [0,1]$ defines an attenuate factor to discount the importance of common neighbors far away. 

The accuracy of a global index is given on the expense of scalability. Since the global indices takes the topology of the whole network into account, the measures can be very time-consuming and thus inappropriate for large-scale networks. 

There is a tradeoff called quasi-local indices. As the name indicated, it is based on more information than the local indices but eliminates nodes too far away in the network. According to \citet{lu2011}, the result is less time consuming algorithms with higher accuracy than the local indices, since superfluous information, contributing with little improvement in accuracy, is ignored.

\textbf{Local Path Index} The Local Path (LP) Index is a quasi-local similarity index which provides a good tradeoff between the accuracy and computational complexity \citep{lu2011}. It is defined as 
\begin{equation}
    \label{lp}
    \sigma_{LP} = \textbf{A}^2+\alpha \textbf{A}^3
\end{equation}
where $\textbf{A}$ denotes the adjacency matrix of the graph. Taking the square of the adjacency matrix gives the number of paths there is between node $i$ and node $j$ with length 2. Moreover, $\textbf{A}^3$ given the number of paths between two nodes with length 3. 

$\alpha$ is a discounting factor which relates to the importance of having the same neighbour two steps away. Thus, a high value of 1 indicates high importance, making a common neighbor two steps away as important as having a common nearest neighbor. Setting $\alpha$ to 0 degenerates the expression to only account for the nearest neighbors, thus making the Local Path Index equal $\sigma_{common}$, see equation \eqref{common}.

\section{Centrality}
By exploiting the structure of a graph or a subgraph, the \textit{centrality} of a node can be determined. 

Centrality is a measure that is calculated on undirected graphs. Dealing with directed graphs, this similar measures are instead called \textit{prestige} or \textit{importance} measures. \cite{fouss2016algorithms}


\subsection{Closeness Centrality}
The closeness centrality measure indicates the proximity of a node $i$ to a node $j$ in an undirected graph $G$. The measure implies to what extent node $i$ is central to $G$, hence, how representative it is to the network \citep{fouss2016algorithms}. The node with the highest centrality score is the most central node.

According to \citet{fouss2016algorithms}, the most popular choice for quantifying the closeness centrality is
\begin{equation}
    cc_i=\frac{1}{\sum_{j=1}^{n} \Delta_{ij}}
\end{equation}
where $\Delta_{ij}$ denotes the shortest-path distance between the nodes $i$ and $j$, and $n$ is the number of nodes in the graph. The most central node is the one closest to all the rest of the nodes. For the case with an unweighted, undirected graph, the highest closeness centrality is found for a node adjacent to all the rest. The closeness centrality is then given by $\frac{1}{n-1}$.

\subsection{Betweenness Centrality}
Betweenness centrality can be used to quantify to what extent a node is an important intermediary \citep{fouss2016algorithms}. Nodes with high betweenness are very important since many of the other nodes communicates through them \citep{Kajdanowicz2013}. 

The best-known walkbased centrality measure is the Freeman's betweenness centrality \citep{fouss2016algorithms}. It is also known as the shortest-path betweenness centrality and is computed by taking the shortest paths $p_{ik}^*$ going through an intermediate node $j$, where $i\neq j \neq k\neq i$ for all the node pairs $i$ and $k$. The shortest path between node $i$ and $k$ as $p_{ik}^*$ is then a subset of all the shortest paths denoted as $P_{ik}^*$ connecting node $i$ and $k$. Thus, the number of shortest paths between the two nodes are given by $|P_{ik}^*|$. Moreover, we denote the total number of shortest paths between node $i$ and $k$ going through node $j$ as $\eta(j \in P_{ik}^*)$. In accordance with the notation used in \citep{fouss2016algorithms} we can now formulate the shortest-path betweenness centrality as follows
\begin{equation}
    \text{bet}_j=\sum_{i=1, i\neq j}^{n} \sum_{k=1, k \neq i,j}^{n} \frac{\eta(j \in P_{ik}^*)}{|P_{ik}^*|}
\end{equation}
A more intuitively way of putting it is that the betweenness centrality measures how many times node $j$ is on the shortest path between two nodes in the graph. Hence, it gives an indication of important the node is for the communication of a network. 

\section{Prestige}
Prestige or importance measures are very similar to the concept of centrality but differs in the sense that it is applied on directed graphs instead of undirected graphs. 

\subsection{PageRank}
The PageRank algorithm was developed in 1998 in order to rank web pages and is one of the algorithms Google currently uses in the Google search engine \cite{langville2004deeperinside,langville2012}. The PageRank score is based on the the number of times a node is cited by other nodes, taking the node's importance into account. The PageRank algorithm assigns a prestige score to every node in the network. According to \citet{fouss2016algorithms}, the prestige score can in its most basic form be calulcated as 
\begin{equation}
    x_j=\sum_{i=1}^{n} \frac{a_{ij}x_i}{a_{i\circ}}
\end{equation}
where $x_i$ is the prestige score associated with a node $i$ and $a_{ij}$ is an element in the non-symmetric adjacency matrix $\textbf{A}$, and $a_{i\circ}$ denotes the outdegree of node $i$. 

As a result, a node $j$ receives a high PageRank score if it is related to many other nodes, if the nodes adjacent to $j$ have high prestige scores themselves and the neighbors to node $j$ have a small outdegree. 

% Difference between PageRank and HITS. Refer to langville2004; HITS is more costly

\section{Support Vector Machine}
Support Vector Machines (SVM) is a set of supervised learning methods originally developed for binary classification \citet{Hsu2002}. In this section, we will only briefly describe the theory behind SVM. For a more rigorous description, we refer to \citet{Kecman2005} or \citet{Cortes1995}.

Given training vectors $\bm{x}_i\in \mathds{R}^p$ where $i=1,...,n$, and a target vector $y \in \{-1,1\}^n$, the SVM solves the following problem \citep{Hsu10apractical}
\begin{align}
    \nonumber
    \min_{\bm{w},b,\bm{\xi}}\text{~~}&\frac{1}{2}\bm{w}^T\bm{w}+C\sum_{i=1}^{n} \xi_i \\ 
    \text{subject to~~}&y_i(\bm{w}^T\phi(\bm{x}_i)+1)\geq 1-\xi_i\\ 
    &\xi_i\geq 0 \nonumber
\end{align}
where $\phi(\bm{x}_i)$ is a mapping of $\bm{x}_i$ to a higher dimension and $C>0$ is the penalty parameter for the error term. Solving this problem will find you a hyperplane to set the boundary decision line in the high dimensional space $\phi$.

% No overfitting due to the regularization parameter C

\subsection{One-against-one}
Much research have been devoted towards multi-class SVM. \citet{Hsu2002} have made a comparison of the one-agains-all, one-against-one and Directed Acyclic Graph Support Vector Machines (DAGSVM) methods. In their paper, \citet{Hsu2002} concludes that one of the more suitable methods for practical use is the one-agains-one.

One-against-one constructs one SVM for each pair of classes. Hence, a problem with $c$ classes would impose $c(c-1)/2$ SMVs.

\subsection{Scaling \label{scale}}
\citet{Hsu10apractical} addresses the issue of varying magnitudes of the attributes in $\bm{x}_i$. To avoid that attributes of higher order of magnitude dominates other attributes, all features should be scaled. As a result, another advantage arises since scaling the values avoids any numerical difficulties. \citet{Hsu10apractical} recommend to normalize the values to the range [-1,1] or [0,1].


\section{Prediction and Recall}
To evaluate the performance of a binary classifier, the concepts of precision and recall are useful. Precision measures the retrieved instances that are relevant while recall measures how many of the relevant instances that are retrieved. Another way of putting it is to say that precision is a measure of confidence while recall gives the sensitivity \citep{powers2011}.

The measurements are given calculated from the number of True Positives (TP), False Positives (FP) and False Negative (FN). Prediction and recall are calculated by \citep{powers2011}
\begin{align}
\text{Precision}&=\frac{TP}{TP+FP}\\
\text{Recall}&=\frac{TP}{TP+FN}
\end{align}
To be exhaustive in the notation, there is also the case of True Negative (TN) although it is never used in the computation.

To evaluate binary classifiers, the Area Under Curve (AUC) can be calculated. AUC$\in[0,1]$ where a high value reflects a good classifier. The AUC is simply calculated as the integral of the prediction-recall curve.

Furthermore, there is the F$_1$ score reflecting the classifiers accuracy. The F$_1$ score is calculated as the harmonic mean of precision and recall \citep{powers2011}
\begin{equation}
    \text{F}_1 = 2\frac{\text{Precision}\cdot \text{Recall}}{\text{Precision}+\text{Recall}}
\end{equation}

\section{Cross-validation}
Cross-validation (CV) is a validation technique to assess how a statistical method would perform on an independent data set. Thus, CV evaluates the generalization of the model without the need for the three data sets; training, validation and test.

CV is based on two data sets, one training set and one test set for the final evaluation of the predicative model. Thus, CV is useful in the case of limited data. 

One of the basic approaches is called the k-fold CV. The data set is divided randomly into $k$ equally sized bins. The model is trained using the $k-1$ fold of data while the last one is used for testing. This procedure is then repeated many times over and the final performance is given as the average. \citet{Krstajic2014} have evaluated the k-fold CV (in their article referring to it as the V-fold CV) with 10 folds and 50 repetitions. 




%%%%% OLD STUFF %%%%
\begin{comment}
\section{Attack Graphs}
Attack Graphs are graphs where vulnerabilities and exploits are represented, thus making it a tool to assess the security of enterprise networks \cite{barik2016}. The concept of attack graphs was introduced in 1998 

\subsection{Regular equivalence}
In some cases it is of interest to understand the similarity in position that nodes represent in a network.  It is then useful to study the regular equivalence between nodes. In the context of a social network of a company, two nodes representing managers over two different departments should according to regular equivalence give a high value of similarity. 

In other words, two nodes are regularly equivalent if they are equally related to equivalent others. This implies an iterative or recursive nature since the similarity between the neighborhoods of the nodes has to be known before the similarity of the nodes themselves can be computed \cite{leicht2006}. 

One way of retrieving an exact solution of the regular equivalence is the 
One algorithm to determine the regular equivalence is REGE. 

While REGE is applicable to quantitative data, CATREGE is used for categorical data. 

\section{Kernels on a Graph}

Kernels can be used on graphs to capture the similarity between two nodes or between two disjoint subgraphs. The kernel takes all paths into consideration; both indirect and direct paths. They have the property of increasing the element when the number of paths connecting two nodes are many and the length of the paths decreases. 

A kernel is a function that maps two objects to a real number to represent the similarity between the two objects. More precisely, it is a function $k(i,j):\Omega \times \Omega \rightarrow \mathbb{R}$, where the two objects $i,j\in \Omega$ are defined in some input space $\Omega$, that return a similarity measure \cite{fouss2016algorithms}. 

A simple, classical similarity measure is obtained by taking the inner product of the node vectors $x_i$ ans $x_j$. A kernel function is symmetric and positive semidefinite. 

\cite{gartner2008kernels}

\citet{kondor2002diffusionkernels} has defined the exponential diffusion kernel as
\begin{equation}
    \textbf{K} \triangleq \sum_{t=0}^{\infty} \frac{\alpha^t \textbf{A}^t}{t!} = e^{\alpha \textbf{A}}
\end{equation}
where $t$ is the number of transitions away from a specific node, $\textbf{A}$ is the adjacency matrix and the elements $a_{ij}$ represent the direct paths between nodes $i$ and $j$. $\alpha \in (0,1)$ is a discounting factor, where a small value represents small importance of nodes far away, e.g. a high number of transitions away. Thus, the kernel favors shorter paths by giving them a heavier weight.
\end{comment}