\chapter{Results}
In this chapter our results are presented. Starting off with how we solved the challenge of extracting relations from Recorded Future's database, we move on to the results of the three cases presented in \secref{cases}. 

\section{Extracting data from database}
We decided to extract data once from the Recorded Future database and store it in a graph database where relations are accessed in constant look up time. Due to the above reasoning we opted to use Neo4j Graph Database, which supports some basic analysis through their query language Cypher.

In order to get the data from Recorded Future's databse and store it in Neo4j we developed a software that builds upon the existing Recorded Future API. The software lets a user specify nodes and relations, through JSON query, from a Recorded Future API query response. An example query can be seen in \secref{ap:query}.

\section{Detection of Gh0st RAT Controllers}
In this section, the results from the Gh0st RAT case described in \secref{methodGhost}. We present the AUC values and the maximal measured F$_1$ score for the different indices and different numbers of reference nodes that the similarity is based on. The results from the first dataset is presented and then the results from the second dataset. Since cross validation is performed, 20 trials has been carried out. However, we first motivate out choice of parameters for the Local Path index and Katz index.

\textbf{Choice of parameters.} For the Local Path index and Katz index,  the parameter $\alpha$ had to be chosen. To do so, different values of alpha were tried and evaluated based on the resulting AUC and F$_1$ values. For the Local Path index, the performance was lowered with an increasing value of $\alpha$. Thus, to make the most fair comparison, a low value of alpha was chosen, namely 0.01. For the global index, the Katz index, a maximization of performance was found for $\alpha=0.5$. These values of the discounting factor $\alpha$ were used throughout the simulations. 

\textbf{The first dataset.} The result for the first dataset in the RAT controller case is given in table \ref{aucIndex} and figure \ref{gh0stChart1}. We find small variance in the results for the local indices. The quasi-local Local Path index and the global Katz index perform worse than the local ones. The best overall performance is found when using 4-5 reference nodes. The best F$_1$ value is then 98\% and an AUC of approximately 96\%. 

\begin{table}[h!]
    \centering
    \caption{AUC and F$_1$ score for different number of reference nodes. The AUC is calculated from the precision-recall curve. The F$_1$ score represents the average accuracy between the 20 runs.}
    \begin{tabular}{|c||c|c||c|c||c|c||c|c||c|c|}
      \hline
      \multirow{2}{*}{~} 
            & \multicolumn{2}{c||}{1}
            & \multicolumn{2}{c||}{2}
            & \multicolumn{2}{c||}{3}
            & \multicolumn{2}{c||}{4}
            & \multicolumn{2}{|c|}{5} \\             \cline{2-11}
      ~     &AUC&F$_1$&AUC&F$_1$&AUC&F$_1$&AUC&F$_1$&AUC&F$_1$ \\ \hline
    Dice    &0.8995 & 0.89 & 0.9363 &0.94 &0.9554&0.97 & 0.9627 &0.98&0.9658 & 0.98 \\
    Jaccard &0.8978 & 0.89 & 0.9367 &0.94 &0.9552&0.97 & 0.9616 &0.98&0.9654 & 0.98 \\
    Cosine  &0.8943 & 0.89 & 0.9337 &0.94 &0.9518&0.97 & 0.9591 &0.98&0.9621 & 0.98 \\
    ProHub  &0.8850 & 0.89 & 0.9244 &0.94 &0.9435&0.97 & 0.9478 &0.98&0.9434 & 0.98 \\
    DeHub   &0.8987 & 0.89 & 0.9378 &0.94 &0.9563&0.97 & 0.9631 &0.98&0.9664 & 0.98 \\
    LP      &0.6169 & 0.52 & 0.8365 & 0.85 & 0.8505 & 0.88 & 0.9024 & 0.92 & 0.9009 & 0.91 \\ 
    Katz    &0.2233 & 0.11 & 0.2336 & 0.11 & 0.1958 & 0.11 & 0.1781 & 0.11 & 0.1566 & 0.20 \\ \hline
    \end{tabular}
    \label{aucIndex}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{gh0st1Chart.eps}
    \caption{AUC and F1-score of different indices on the first dataset of the Gh0st RAT dataset.}
    \label{gh0stChart1}
\end{figure}


Furthermore, there were three IP addresses that were constantly classified as RAT controllers, while they in fact were annotated as \textit{Non-gh0st}s. Performing 100 trials for each case of using 1-5 reference nodes, the IP addresses were classified as RAT controllers in the majority or all of the times. Using only one reference node, the three different IP addresses were classified as RAT controller in 76\%, 97\% and 95\% of the times, respectively. Using two reference nodes, they were classified as RAT controller in 95\%, 99\% and 99\% of the times. Using 3-5 reference nodes, all three IP addresses were classified as RAT controllers in every one of the 100 trials. Because of this, there exists reason to believe they are incorrectly annotated. 

\textbf{The second dataset.} The second data set was much more sparse i terms of RAT controllers. Out of 10,091 nodes, only 32 were identified as RAT controllers. Performing the exact same analysis on this data set as the previous one gave us the result shown in table \ref{aucIndex2} and in figure \ref{gh0stChart2}.

\begin{table}[h!]
    \centering
    \caption{AUC and F$_1$ score for different number of reference nodes. The AUC is calculated from the precision-recall curve. The F$_1$ score represents the average accuracy between the 20 runs.}
    \begin{tabular}{|c||c|c||c|c||c|c||c|c||c|c|}
      \hline
      \multirow{2}{*}{~} 
            & \multicolumn{2}{c||}{1}
            & \multicolumn{2}{c||}{2}
            & \multicolumn{2}{c||}{3}
            & \multicolumn{2}{c||}{4}
            & \multicolumn{2}{|c|}{5} \\             \cline{2-11}
      ~     &AUC&F$_1$&AUC&F$_1$&AUC&F$_1$&AUC&F$_1$&AUC&F$_1$ \\ \hline
    Dice    & 0.9696 & 0.98 & 0.9680 & 1.00 & 0.9531 & 1.00 & 0.9375 & 1.00 & 0.9219 & 1.00 \\
    Jaccard & 0.9696 & 0.98 & 0.9680 & 1.00 & 0.9531 & 1.00 & 0.9375 & 1.00 & 0.9219 & 1.00 \\
    Cosine  & 0.9696 & 0.98 & 0.9680 & 1.00 & 0.9531 & 1.00 & 0.9375 & 1.00 & 0.9219 & 1.00 \\
    ProHub  & 0.9696 & 0.98 & 0.9680 & 1.00 & 0.9531 & 1.00 & 0.9375 & 1.00 & 0.9297 & 1.00 \\
    DeHub   & 0.9696 & 0.98 & 0.9680 & 1.00 & 0.9531 & 1.00 & 0.9375 & 1.00 & 0.9219 & 1.00 \\ 
    LP      & 0.7386 & 0.88 & 0.7970 & 0.86 & 0.7574 & 0.92 & 0.4574 & 0.95 & 0.1490 & 0.96 \\
    Katz    & 0.0982 & 0.15 & 0.1426 & 0.26 & 0.1742 & 0.35 & 0.2569 & 0.37 & 0.2660 & 0.37 \\ \hline
    \end{tabular}
    \label{aucIndex2}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{gh0st2Chart.eps}
    \caption{AUC and F1-score of different indices on the second dataset of the Gh0st RAT dataset.}
    \label{gh0stChart2}
\end{figure}

In the second dataset, no incorrectly annotated IP addresses were found. 

\nopagebreak
\FloatBarrier
\section{Classification of malicious IP addresses}
Below, the results of the IP feature classifier described in \secref{methodSVM} is presented. First, the results from the random classifier is presented as a reference for the SVM models. Thereafter, the results from the SVM models, based on different features are presented. The results are based on 50 repetitions and are given with a confidence level of 95\%.

\textbf{Random classifier.} For a pseudorandom classifier, the results can be found in table \ref{randClass}. An accuracy of 20\% was found, which according to theory was to be expected since there are 5 difference classes. However, 70\% of the predictions gave a higher risk classification and 10\% gave a lower risk classification.

\begin{table}[h!]
    \centering
    \caption{Results from the a random classifier, based on a uniform distribution. The table shows the fraction of correct predictions (accuracy), the fraction of higher predictions and the fraction of lower predictions in relation to Recorded Future's actual classification.}
    \begin{tabular}{|c|c|}
    \hline
        \multicolumn{2}{|c|}{Random classifier}\\ \hline
        Correct classification & 0.2002 $\pm$ 0.0022 \\
        Higher classification  & 0.6994 $\pm$ 0.0023\\
        Lower classification   & 0.1003 $\pm$ 0.0017\\ \hline
    \end{tabular}
    \label{randClass}
\end{table}



\textbf{Two features.} Using only the two features PageRank and degree, the best accuracy was found for $C=10$ and $\gamma=10$. The results from including only the two topological features are presented in table \ref{IpRes2Feat}. The SVM classifier, based on 2 input features, performed prediction with an accuracy of 32.2\%, which is a performance slightly better than the random classifier.

\begin{table}[h!]
    \centering
    \caption{Results from the SVM classifier using two features. The table shows the fraction of correct predictions (accuracy), the fraction of higher predictions and the fraction of lower predictions in relation to Recorded Future's actual classification.}
    \begin{tabular}{|c|c|}
    \hline
        \multicolumn{2}{|c|}{SVM with 2 features}\\ \hline
        Correct classification  & 0.3223 $\pm$ 0.0148 \\
        Higher classification   & 0.5344 $\pm$ 0.0181 \\
        Lower classification    & 0.1432 $\pm$ 0.0038 \\ \hline
    \end{tabular}
    \label{IpRes2Feat}
\end{table}


\textbf{Four features.} Using four features in the model, the best accuracy was found for $C=100$ and $\gamma=100$. The results from including only four features are presented in table \ref{IpRes4Feat}. The features included information about the number of hits and the neighbours co-occurrence with malwares, cyber vulnerabilities and attack vectors.  The SVM classifier, based on 4 input features, performed prediction with an accuracy of 50.3\%.

\begin{table}[h!]
    \centering
    \caption{Results from the SVM classifier using four features. The table shows the fraction of correct predictions (accuracy), the fraction of higher predictions and the fraction of lower predictions in relation to Recorded Future's actual classification.}
    \begin{tabular}{|c|c|}
    \hline
        \multicolumn{2}{|c|}{SVM with 4 features}\\ \hline
        Correct classification  & 0.5031 $\pm$ 0.0147 \\
        Higher classification   & 0.4233 $\pm$ 0.0202 \\
        Lower classification    & 0.0736 $\pm$ 0.0056 \\ \hline
    \end{tabular}
    \label{IpRes4Feat}
\end{table}


\textbf{All six features.} Including all the six features in the model, the best accuracy was found for the parameters $C=10$ and $\gamma=50$. The results for the SVM, using six features, are presented in table \ref{IpRes}. The SVM classifier, based on all 6 input features, performed prediction with an accuracy of 56.3\%.

\begin{table}[h!]
    \centering
    \caption{Results from the SVM classifier using six features. The table shows the fraction of correct predictions (accuracy), the fraction of higher predictions and the fraction of lower predictions in relation to Recorded Future's actual classification.}
    \begin{tabular}{|c|c|}
    \hline
        \multicolumn{2}{|c|}{SVM with 6 features}\\ \hline
        Correct classification  & 0.5629 $\pm$ 0.0077 \\
        Higher classification   & 0.3564 $\pm$ 0.0115 \\
        Lower classification    & 0.0807 $\pm$ 0.0043 \\ \hline
    \end{tabular}
    \label{IpRes}
\end{table}


\textbf{Accuracy per class.} A segmentation of accuracy per class was also performed, in accordance with the discussion in \secref{unbalanced}. The results of the accuracies for the three different SVM models, are presented in figure \ref{accuracyPerClass}. Noticeable is the low accuracy of class 2 in every model. Furthermore, we find the model using 6 features to have the most evenly distributed accuracy among the classes. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{resultIPperClass.eps}
    \caption{Accuracy per class for the SVM classifiers based on 2, 4 and 6 features, respectively.}
    \label{accuracyPerClass}
\end{figure}

\FloatBarrier
\section{Prediction of Future Cyber Attacks}
\input{chapters/cyberattacks_result}
