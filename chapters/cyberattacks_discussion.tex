% Incorrectness of results due to incorrectness in DB
% Running time
% Limitations because of earlier attacks and new actors
% Maximum prediction rate
% Length of train periods

Before discussing the results of the PLP algorithm on the cyber attack data set it is worth mentioning that the predictions made does not necessarily indicate that attack is likely to happen. This is because the dataset is based of mentions on the internet of cyber attacks, what is being predicted is the future possibility of mentions of cyber attacks such that it is recorded and added to the Recorded Future Database. Such recordings might be incorrect since NLP together with other algorithms is used to harvest the information from the sources.

However, the accuracy of the predictions is for all time periods tested impressing. The prediction rate is naturally limited by that it is not possible to predict attacks involving actors (targets or attackers) not already in the data set used for prediction. There is also a limit because the algorithm does not give a predictive index for attacks that consists of the same set of actors that has previously been involved together in a previous attack. The maximum prediction rate possible due to the factors just discussed varied between 60\%-80\%. In \figref{fig:plp_max} we see that the largest contributing factor is the amount of cybert attacks consisting of a pair of actors already together in a cyber attack in the prediction set. Naturally it is increasing when the prediction set grows.

With the limiting factors in mind, a prediction rate of 20\%-25\% together with the high accuracy of the predictions is good enough for the algorithm to be useful in the applied work of predicting cyber attacks. All factors in my, we would in this case recommend the set used for prediction to be 6 since it offers a strong AUC as well as prediction rate and maximum prediction rate. It also reduces the running time of the algorithm if the prediction data set is small.

The most likely reason for the prediction rate not being higher is probably because graph representing the data set contains a lot of isolated sub graphs that limits the algorithm ability. Then there is of course the factor that threat actors act unpredictably which is the reason why companies trying to counter cyber attacks invests a lot of time and resources on intelligence work.

What is most interesting is that ~20\% of cyber attacks can be predicted just by using the available topological information in the bipartite graph without using any domain knowledge.

To further improve the algorithm it would be interesting to see if it would be possible to predict new attacks that has already happened once before. One way of doing has already been done by reducing the length of the prediction data set.

One method that we tried but had no positive impact was to add the number of references of an attack to the weight of the projection. In the tested, it actually had a negative impact.

In many cases the interest of predicting future cyber attacks stem from the need to prepare and prevent such attack. The name of the attacker might then only be of secondary interest. The primary interest is to find out what methods might be used and what vulnerabilities might be exploited. With this in mind the bipartite graph could be constructed by a set of attacker profiles and target name instead of attacker name and target name. The profiling could be based on what vulnerabilities and methods the attacker has previously used, together wish other domain specific information. This would reduce the number of vertices in the graph and make it denser. It would also enable to predict attacks with attackers not already in the database if the future attacker fits a profile in the database.
