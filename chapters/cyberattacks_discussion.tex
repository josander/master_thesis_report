% Incorrectness of results due to incorrectness in DB
% Running time
% Limitations because of earlier attacks and new actors
% Maximum prediction rate
% Length of train periods

Before discussing the results of the PLP algorithm on the cyberattack dataset it is worth mentioning that the predictions made do not necessarily indicate that attack is likely to happen. This is because the dataset is based of mentions on the internet of cyberattacks, what is being predicted is the future possibility of mentions of cyberattacks such that it is recorded and added to the Recorded Future Database. Such recordings might be incorrect since NLP together with other algorithms is used to harvest the information from the sources.

However, the accuracy of the predictions is for all time periods tested impressing. The prediction rate is naturally limited by that it is not possible to predict attacks involving actors (targets or attackers) not already in the dataset used for prediction. There is also a limit because the algorithm does not give a predictive index for attacks that consists of the same set of actors that have previously been involved together in a previous attack. The maximum prediction rate possible due to the factors just discussed varied between 60\%-80\%. In \figref{fig:plp_max} we see that the largest contributing factor is the amount of cybert attacks consisting of a pair of actors already together in a cyberattack in the prediction set. Naturally it is increasing when the prediction set grows.

With the limiting factors in mind, a prediction rate of 20\%-25\% together with the high accuracy of the predictions is good enough for the algorithm to be useful in the applied work of predicting cyberattacks. All factors in mind, we would in this case recommend the set used for prediction to be 6 months since it offers a strong AUC as well as prediction rate and maximum prediction rate. It also reduces the running time of the algorithm if the prediction dataset is small.

The most likely reason for the prediction rate not being higher is probably because the graph representing the dataset contains a lot of isolated sub graphs that limits the algorithm ability. Then there is of course the factor that threat actors act unpredictably which is the reason why companies trying to counter cyberattacks invest a lot of time and resources on intelligence work.

What is most interesting is that ~20\% of cyberattacks can be predicted just by using the available topological information in the bipartite graph without using any domain knowledge.

To further improve the algorithm it would be interesting to see if it could be possible to predict new attacks that have already happened once before. One way of doing it, has already been done by reducing the length of the prediction dataset, since that excludes many earlier attacks.

One method that we tried, that had no positive impact was to add the number of references of an attack to the weight of the projection. When tested, it actually had a negative impact.

In many cases the interest in predicting future cyberattacks stems from the need to prepare and prevent such attacks. The name of the attacker might then only be of secondary interest. The primary interest is to find out what methods might be used and what vulnerabilities might be exploited. With this in mind the bipartite graph could be constructed by a set of attacker profiles and target name instead of attacker name and target name. The profiling could be based on what vulnerabilities and methods the attacker has previously used, together with other domain specific information. This would reduce the number of vertices in the graph and make it denser. It would also enable predicts of attacks with attackers not already in the database if the future attacker fits a profile in the database.
